{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFJGvWc0sZu_"
      },
      "source": [
        "1. Задача обучить Pong с помощью DQN, используя только полносвязанные слои, оставив только один канал.\n",
        "2. Число эпизодов и других параметров обучения выбираются в соответствии с вычислительной мощностью."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4X6vRLF04fI",
        "outputId": "bfd2a2ad-eedf-4101-cd93-c109c44cfaa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.10.1)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gymnasium[atari]\"\n",
        "!pip install autorom[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hZMc0al1s6v"
      },
      "outputs": [],
      "source": [
        "# Настройка среды Atari\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "import torch                      # библиотека для работы с нейронными сетями\n",
        "import torch.nn as nn             # модуль для создания нейронных сетей\n",
        "import torch.optim as optim       # модуль для оптимизации нейронных сетей\n",
        "import torch.nn.functional as F   # модуль с функциями активации и потерь\n",
        "import numpy as np                # библиотека для работы с массивами\n",
        "from collections import deque     # структура данных для буфера воспроизведения\n",
        "import random                     # модуль для генерации случайных чисел\n",
        "import matplotlib.pyplot as plt   # библиотека для построения графиков\n",
        "import os                         # модуль для работы с файловой системой\n",
        "import base64                     # модуль для кодирования и декодирования данных\n",
        "import copy                       # копирование объектов\n",
        "\n",
        "# Поддерживает обратное распространение\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Модуль для отображения видео в Google Colab\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Перевод среды в черно-белое\n",
        "from gymnasium.wrappers import GrayscaleObservation\n",
        "from gymnasium.wrappers import FlattenObservation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b5TM-Lf05fs",
        "outputId": "26157d0c-64ca-4418-e808-4ebd118ff0b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/random-video-folder folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.envs.make(\"PongDeterministic-v4\", render_mode='rgb_array')\n",
        "\n",
        "# Видео\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    episode_trigger=lambda num: num % 100 == 0,\n",
        "    video_folder=\"random-video-folder\",\n",
        "    name_prefix=\"video-\",\n",
        ")\n",
        "\n",
        "# Переводим среду в черно-серый цвет\n",
        "grayscale_env = GrayscaleObservation(env)\n",
        "\n",
        "# Выравнивание среды\n",
        "flatten_env = FlattenObservation(grayscale_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK9K-Cpn07G0",
        "outputId": "b37e50b0-82ca-45b3-ef1f-c5d07283af75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Число состояний (форма): (33600,)\n",
            "Число действий: 6\n",
            "Доступные действия: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ]
        }
      ],
      "source": [
        "state_shape = flatten_env.observation_space.shape\n",
        "print('Число состояний (форма):', state_shape)\n",
        "n_action = flatten_env.action_space.n\n",
        "print('Число действий:', n_action)\n",
        "\n",
        "print('Доступные действия:', flatten_env.unwrapped.get_action_meanings())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcttWIIEdTU2"
      },
      "outputs": [],
      "source": [
        "ACTIONS = [0, 2, 3] # 'NOOP', 'RIGHT', 'LEFT'\n",
        "n_action = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWZhg-myGPP0"
      },
      "outputs": [],
      "source": [
        "class DQN():\n",
        "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.05):\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_state, n_hidden),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(n_hidden, n_action)\n",
        "        )\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "\n",
        "        # Инициализируем целевую сеть\n",
        "        self.model_target = copy.deepcopy(self.model)\n",
        "\n",
        "    # Метод для синхронизации весов целевой и предсказательной сетей\n",
        "    def copy_target(self):\n",
        "        self.model_target.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    # метод обучения, который обновляет нейронную сеть, получив новый пример\n",
        "    def update(self, s, y):\n",
        "        \"\"\"\n",
        "        Обновляет веса DQN, получив обучающий пример\n",
        "        @param s: состояние\n",
        "        @param y: целевое значение\n",
        "        \"\"\"\n",
        "        y_pred = self.model(torch.Tensor(s))\n",
        "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    # Вычисление ценностей с помощью целевой сети\n",
        "    def target_predict(self, s):\n",
        "        \"\"\"\n",
        "        Вычисляет значения Q-функции состояния для всех действий\n",
        "        с помощью целевой сети\n",
        "        @param s: входное состояние\n",
        "        @return: целевые ценности состояния для всех действий\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model_target(torch.Tensor(s))\n",
        "\n",
        "    # Для вычисления целевой ценности будем использовать целевую,\n",
        "    # а  не предсказательную сеть:\n",
        "    def replay(self, memory, replay_size, gamma):\n",
        "        \"\"\"\n",
        "        Буфер воспроизведения совместно с целевой сетью\n",
        "        @param memory: буфер воспроизведения опыта\n",
        "        @param replay_size: сколько примеров использовать при каждом\n",
        "        обновлении модели\n",
        "        @param gamma: коэффициент обесценивания\n",
        "        \"\"\"\n",
        "        if len(memory) >= replay_size:\n",
        "            replay_data = random.sample(memory, replay_size)\n",
        "            states = []\n",
        "            td_targets = []\n",
        "            for state, action, next_state, reward, is_done in replay_data:\n",
        "                states.append(state)\n",
        "                q_values = self.predict(state).tolist()\n",
        "                if is_done:\n",
        "                    q_values[action] = reward\n",
        "                else:\n",
        "                    q_values_next = self.target_predict(next_state).detach()\n",
        "                    q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
        "                td_targets.append(q_values)\n",
        "\n",
        "            self.update(states, td_targets)\n",
        "\n",
        "    # Функция предсказания ценности состояния:\n",
        "    def predict(self, s):\n",
        "        \"\"\"\n",
        "        Вычисляет значения Q-функции состояния для всех действий,\n",
        "        применяя обученную модель\n",
        "        @param s: входное состояние\n",
        "        @return: значения Q для всех действий\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.model(torch.Tensor(s))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5aHoM7JAcbz"
      },
      "outputs": [],
      "source": [
        "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
        "    def policy_function(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, n_action - 1)\n",
        "        else:\n",
        "            q_values = estimator.predict(state)\n",
        "        return torch.argmax(q_values).item()\n",
        "    return policy_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgo9E6rfAWuj"
      },
      "outputs": [],
      "source": [
        "# n_action = 3\n",
        "n_state = flatten_env.observation_space.shape[0]\n",
        "n_hidden = 50\n",
        "lr = 0.005\n",
        "\n",
        "target_update = 10 # целевая сеть обновляется после каждых 10 эпизодов\n",
        "\n",
        "n_episode = 50\n",
        "replay_size = 32 # размер выборки из буфера воспроизведения на каждом шаге\n",
        "\n",
        "# Будем запоминать полные вознаграждения в каждом эпизоде\n",
        "total_reward_episode = [0] * n_episode\n",
        "\n",
        "dqn = DQN(n_state, n_action, n_hidden, lr)\n",
        "\n",
        "# Буфер для хранения опыта\n",
        "memory = deque(maxlen=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBSTjAsUARRT"
      },
      "outputs": [],
      "source": [
        "def q_learning(env, estimator, n_episode, replay_size, target_update=10, gamma=1.0, epsilon=0.1, epsilon_decay=.99):\n",
        "    \"\"\"\n",
        "    Глубокое Q-обучение методом Double DQN с воспроизведением опыта\n",
        "    @param env: имя окружающей среды Gym\n",
        "    @param estimator: объект класса DQN\n",
        "    @param replay_size: сколько примеров использовать при каждом\n",
        "    обновлении модели\n",
        "    @param target_update: через сколько эпизодов обновлять целевую сеть\n",
        "    @param n_episode: количество эпизодов\n",
        "    @param gamma: коэффициент обесценивания\n",
        "    @param epsilon: параметр ε-жад­ной стратегии\n",
        "    @param epsilon_decay: коэффициент затухания epsilon\n",
        "    \"\"\"\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        if episode % target_update == 0:\n",
        "            estimator.copy_target()\n",
        "\n",
        "        policy = gen_epsilon_greedy_policy(estimator, epsilon, n_action)\n",
        "        state, _ = env.reset()\n",
        "        # state = get_state(obs)\n",
        "\n",
        "        while True:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, truncated, _ = env.step(ACTIONS[action])\n",
        "            total_reward_episode[episode] += reward\n",
        "\n",
        "            # next_state = get_state(next_obs)\n",
        "            memory.append((state, action, next_state, reward, done or truncated))\n",
        "            if done or truncated:\n",
        "                break\n",
        "            estimator.replay(memory, replay_size, gamma)\n",
        "            state = next_state\n",
        "        print('Эпизод: {}, полное вознаграждение: {}, epsilon:{}'.\n",
        "                    format(episode, total_reward_episode[episode], epsilon))\n",
        "        epsilon = max(epsilon * epsilon_decay, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DUl52tWr4uE",
        "outputId": "f0e0cc44-dac8-4990-ef1a-6b50b473df68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпизод: 0, полное вознаграждение: -21.0, epsilon:0.1\n",
            "Эпизод: 1, полное вознаграждение: -21.0, epsilon:0.099\n",
            "Эпизод: 2, полное вознаграждение: -21.0, epsilon:0.09801\n",
            "Эпизод: 3, полное вознаграждение: -20.0, epsilon:0.0970299\n",
            "Эпизод: 4, полное вознаграждение: -21.0, epsilon:0.096059601\n",
            "Эпизод: 5, полное вознаграждение: -21.0, epsilon:0.09509900499\n",
            "Эпизод: 6, полное вознаграждение: -21.0, epsilon:0.0941480149401\n",
            "Эпизод: 7, полное вознаграждение: -21.0, epsilon:0.093206534790699\n",
            "Эпизод: 8, полное вознаграждение: -21.0, epsilon:0.09227446944279201\n",
            "Эпизод: 9, полное вознаграждение: -21.0, epsilon:0.09135172474836409\n",
            "Эпизод: 10, полное вознаграждение: -21.0, epsilon:0.09043820750088044\n",
            "Эпизод: 11, полное вознаграждение: -21.0, epsilon:0.08953382542587164\n",
            "Эпизод: 12, полное вознаграждение: -21.0, epsilon:0.08863848717161292\n",
            "Эпизод: 13, полное вознаграждение: -21.0, epsilon:0.08775210229989679\n",
            "Эпизод: 14, полное вознаграждение: -21.0, epsilon:0.08687458127689782\n",
            "Эпизод: 15, полное вознаграждение: -21.0, epsilon:0.08600583546412884\n",
            "Эпизод: 16, полное вознаграждение: -21.0, epsilon:0.08514577710948755\n",
            "Эпизод: 17, полное вознаграждение: -21.0, epsilon:0.08429431933839267\n",
            "Эпизод: 18, полное вознаграждение: -21.0, epsilon:0.08345137614500873\n",
            "Эпизод: 19, полное вознаграждение: -21.0, epsilon:0.08261686238355864\n",
            "Эпизод: 20, полное вознаграждение: -21.0, epsilon:0.08179069375972306\n",
            "Эпизод: 21, полное вознаграждение: -21.0, epsilon:0.08097278682212583\n",
            "Эпизод: 22, полное вознаграждение: -21.0, epsilon:0.08016305895390458\n",
            "Эпизод: 23, полное вознаграждение: -21.0, epsilon:0.07936142836436554\n",
            "Эпизод: 24, полное вознаграждение: -21.0, epsilon:0.07856781408072187\n",
            "Эпизод: 25, полное вознаграждение: -21.0, epsilon:0.07778213593991465\n",
            "Эпизод: 26, полное вознаграждение: -21.0, epsilon:0.0770043145805155\n",
            "Эпизод: 27, полное вознаграждение: -21.0, epsilon:0.07623427143471034\n",
            "Эпизод: 28, полное вознаграждение: -21.0, epsilon:0.07547192872036323\n",
            "Эпизод: 29, полное вознаграждение: -21.0, epsilon:0.0747172094331596\n",
            "Эпизод: 30, полное вознаграждение: -21.0, epsilon:0.073970037338828\n",
            "Эпизод: 31, полное вознаграждение: -21.0, epsilon:0.07323033696543972\n",
            "Эпизод: 32, полное вознаграждение: -21.0, epsilon:0.07249803359578533\n",
            "Эпизод: 33, полное вознаграждение: -21.0, epsilon:0.07177305325982747\n",
            "Эпизод: 34, полное вознаграждение: -21.0, epsilon:0.0710553227272292\n",
            "Эпизод: 35, полное вознаграждение: -21.0, epsilon:0.07034476949995691\n",
            "Эпизод: 36, полное вознаграждение: -21.0, epsilon:0.06964132180495734\n",
            "Эпизод: 37, полное вознаграждение: -21.0, epsilon:0.06894490858690777\n",
            "Эпизод: 38, полное вознаграждение: -21.0, epsilon:0.06825545950103869\n",
            "Эпизод: 39, полное вознаграждение: -21.0, epsilon:0.0675729049060283\n",
            "Эпизод: 40, полное вознаграждение: -21.0, epsilon:0.06689717585696801\n",
            "Эпизод: 41, полное вознаграждение: -21.0, epsilon:0.06622820409839833\n"
          ]
        }
      ],
      "source": [
        "q_learning(flatten_env, dqn, n_episode, replay_size, target_update, gamma=.95, epsilon=.1, epsilon_decay=.99)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}